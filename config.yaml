model:
  architecture: "transformer"
  d_model: 512
  batch_size: 30
  ffn_hidden: 2048
  num_heads: 8
  drop_prob: 0.1
  num_layers: 1
  max_sequence_length: 200
  num_layers: 6
  hidden_size: 512
  de_vocab_size: len(german_vocabulary)
  learning_rate: 0.001
  num_epochs: 10
